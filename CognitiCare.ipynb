{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeeb7ec6-3e1b-48e4-bcb0-6ddf9628276e",
   "metadata": {},
   "source": [
    "Data Loading, Cleaning, Preprocessing and Splitting (Categorical Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c84530c-17a7-4d74-a7e4-c79f72944457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Dataset shape: (260951, 194)\n",
      "First 5 rows of the dataset:\n",
      "  diagnostic_summary_changes_phase diagnostic_summary_changes_ptid  rid  \\\n",
      "0                            ADNI2                      003_S_0908    1   \n",
      "1                            ADNI1                      011_S_0010   10   \n",
      "2                            ADNI1                      011_S_0010   10   \n",
      "3                            ADNI1                      011_S_0010   10   \n",
      "4                            ADNI1                      011_S_0010   10   \n",
      "\n",
      "  diagnostic_summary_changes_viscode diagnostic_summary_changes_viscode2  \\\n",
      "0                                v11                                  bl   \n",
      "1                                 bl                                  bl   \n",
      "2                                 bl                                  bl   \n",
      "3                                 bl                                  bl   \n",
      "4                                 bl                                  bl   \n",
      "\n",
      "  diagnostic_summary_changes_examdate  diagnostic_summary_changes_bcadas  \\\n",
      "0                           7/18/2018                                NaN   \n",
      "1                          11/10/2005                               -4.0   \n",
      "2                          11/10/2005                               -4.0   \n",
      "3                          11/10/2005                               -4.0   \n",
      "4                          11/10/2005                               -4.0   \n",
      "\n",
      "   diagnostic_summary_changes_bcmmse  diagnostic_summary_changes_bcmmsrec  \\\n",
      "0                                NaN                                  NaN   \n",
      "1                               -4.0                                 -4.0   \n",
      "2                               -4.0                                 -4.0   \n",
      "3                               -4.0                                 -4.0   \n",
      "4                               -4.0                                 -4.0   \n",
      "\n",
      "   diagnostic_summary_changes_bcnmmms  ...  biomarker_samples_bicsftime  \\\n",
      "0                                 NaN  ...                         -4.0   \n",
      "1                                -4.0  ...                        916.0   \n",
      "2                                -4.0  ...                         -4.0   \n",
      "3                                -4.0  ...                        955.0   \n",
      "4                                -4.0  ...                         -4.0   \n",
      "\n",
      "   biomarker_samples_bicsftrns  biomarker_samples_bicsffroz  \\\n",
      "0                         -4.0                         -4.0   \n",
      "1                        932.0                        933.0   \n",
      "2                         -4.0                         -4.0   \n",
      "3                        956.0                        957.0   \n",
      "4                         -4.0                         -4.0   \n",
      "\n",
      "   biomarker_samples_bilppadate  biomarker_samples_bilpfldate  \\\n",
      "0                            -4                            -4   \n",
      "1                            -4                            -4   \n",
      "2                            -4                            -4   \n",
      "3                            -4                            -4   \n",
      "4                            -4                            -4   \n",
      "\n",
      "   biomarker_samples_bilpspdate  biomarker_samples_id  \\\n",
      "0                          -4.0          24647.779081   \n",
      "1                          -4.0             42.000000   \n",
      "2                          -4.0            482.000000   \n",
      "3                          -4.0           1664.000000   \n",
      "4                          -4.0           4716.000000   \n",
      "\n",
      "   biomarker_samples_siteid  biomarker_samples_userdate  \\\n",
      "0                 37.660482                   7/17/2018   \n",
      "1                107.000000                  11/10/2005   \n",
      "2                107.000000                   5/10/2006   \n",
      "3                107.000000                  11/10/2006   \n",
      "4                107.000000                   11/8/2007   \n",
      "\n",
      "   biomarker_samples_update_stamp  \n",
      "0                         00:00.0  \n",
      "1                         00:00.0  \n",
      "2                         00:00.0  \n",
      "3                         00:00.0  \n",
      "4                         00:00.0  \n",
      "\n",
      "[5 rows x 194 columns]\n",
      "\n",
      "Checking for missing values:\n",
      "biomarker_samples_reason          69.018321\n",
      "mmse_wordlist                     53.757985\n",
      "adas_timebegan                    38.748271\n",
      "adas_timeend                      38.748271\n",
      "adas_q8word24r                    23.857736\n",
      "                                    ...    \n",
      "biomarker_samples_viscode2         0.000383\n",
      "biomarker_samples_examdate         0.000383\n",
      "mmse_ptid                          0.000383\n",
      "mmse_phase                         0.000383\n",
      "biomarker_samples_update_stamp     0.000383\n",
      "Length: 171, dtype: float64\n",
      "Target column 'cognitive_status' not found. Deriving it...\n",
      "Derived column 'cognitive_status' based on selected columns.\n",
      "Dataset shape after dropping rows with missing target: (260951, 195)\n",
      "\n",
      "Numeric columns: Index(['rid', 'diagnostic_summary_changes_bcadas',\n",
      "       'diagnostic_summary_changes_bcmmse',\n",
      "       'diagnostic_summary_changes_bcmmsrec',\n",
      "       'diagnostic_summary_changes_bcnmmms',\n",
      "       'diagnostic_summary_changes_bcneupsy',\n",
      "       'diagnostic_summary_changes_bcnonmem',\n",
      "       'diagnostic_summary_changes_bcfaq', 'diagnostic_summary_changes_bccdr',\n",
      "       'diagnostic_summary_changes_bcdepres',\n",
      "       ...\n",
      "       'biomarker_samples_bilavvol', 'biomarker_samples_bilavfroz',\n",
      "       'biomarker_samples_bicsf', 'biomarker_samples_reason',\n",
      "       'biomarker_samples_bicsftime', 'biomarker_samples_bicsftrns',\n",
      "       'biomarker_samples_bicsffroz', 'biomarker_samples_bilpspdate',\n",
      "       'biomarker_samples_id', 'biomarker_samples_siteid'],\n",
      "      dtype='object', length=152)\n",
      "Categorical columns: Index(['diagnostic_summary_changes_phase', 'diagnostic_summary_changes_ptid',\n",
      "       'diagnostic_summary_changes_viscode',\n",
      "       'diagnostic_summary_changes_viscode2',\n",
      "       'diagnostic_summary_changes_examdate',\n",
      "       'diagnostic_summary_changes_userdate',\n",
      "       'diagnostic_summary_changes_update_stamp', 'adas_phase', 'adas_ptid',\n",
      "       'adas_viscode', 'adas_viscode2', 'adas_visdate', 'adas_q1tr1',\n",
      "       'adas_q1tr2', 'adas_q1tr3', 'adas_q2task', 'adas_q4task', 'adas_q5task',\n",
      "       'adas_q6task', 'adas_q7task', 'adas_userdate', 'adas_update_stamp',\n",
      "       'mmse_phase', 'mmse_ptid', 'mmse_viscode', 'mmse_viscode2',\n",
      "       'mmse_visdate', 'mmse_userdate', 'mmse_update_stamp', 'apoe_phase',\n",
      "       'apoe_ptid', 'apoe_genotype', 'apoe_update_stamp',\n",
      "       'biomarker_samples_phase', 'biomarker_samples_ptid',\n",
      "       'biomarker_samples_viscode', 'biomarker_samples_viscode2',\n",
      "       'biomarker_samples_examdate', 'biomarker_samples_bilppadate',\n",
      "       'biomarker_samples_bilpfldate', 'biomarker_samples_userdate',\n",
      "       'biomarker_samples_update_stamp'],\n",
      "      dtype='object')\n",
      "\n",
      "Final dataset after preprocessing:\n",
      "X_train shape: (208760, 6641), X_test shape: (52191, 6641)\n",
      "y_train distribution:\n",
      "cognitive_status\n",
      "0    139783\n",
      "1     68977\n",
      "Name: count, dtype: int64\n",
      "y_test distribution:\n",
      "cognitive_status\n",
      "0    34947\n",
      "1    17244\n",
      "Name: count, dtype: int64\n",
      "Unique values in the target variable: [1 0]\n",
      "Target variable distribution in training data:\n",
      "cognitive_status\n",
      "0    139783\n",
      "1     68977\n",
      "Name: count, dtype: int64\n",
      "The target variable has multiple classes. Proceeding with the next steps.\n",
      "Performing Variance Threshold to remove low variance features...\n",
      "Shape after Variance Threshold:\n",
      "X_train: (208760, 6432)\n",
      "X_test: (52191, 6432)\n",
      "Applying PCA for dimensionality reduction...\n",
      "Shape after PCA:\n",
      "X_train: (208760, 100)\n",
      "X_test: (52191, 100)\n",
      "Applying SMOTE to address class imbalance...\n",
      "Shape after SMOTE:\n",
      "X_train_balanced: (279566, 100)\n",
      "y_train_balanced distribution:\n",
      "cognitive_status\n",
      "1    139783\n",
      "0    139783\n",
      "Name: count, dtype: int64\n",
      "Preprocessing steps completed. Data saved for model training.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"final_preprocessed_dataset.csv\"\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "print(\"\\nChecking for missing values:\")\n",
    "missing_values = data.isnull().mean() * 100\n",
    "print(missing_values[missing_values > 0].sort_values(ascending=False))\n",
    "\n",
    "# Define the columns to derive the target variable\n",
    "target_column = \"cognitive_status\"  \n",
    "target_columns = [\n",
    "    'diagnostic_summary_changes_bcadas',\n",
    "    'diagnostic_summary_changes_bcmmse',\n",
    "    'diagnostic_summary_changes_bcmmsrec',\n",
    "    'diagnostic_summary_changes_bcnmmms',\n",
    "    'diagnostic_summary_changes_bcnonmem',\n",
    "    'diagnostic_summary_changes_bccdr'\n",
    "]\n",
    "\n",
    "# Define a function to create the target variable\n",
    "def create_target(df, columns):\n",
    "    target = df[columns].apply(lambda x: 1 if 1 in x.values else 0, axis=1) # Check if any of the specified columns contain the value 1\n",
    "    return target\n",
    "\n",
    "if target_column not in data.columns:\n",
    "    print(f\"Target column '{target_column}' not found. Deriving it...\")\n",
    "    data[target_column] = create_target(data, target_columns)\n",
    "    print(f\"Derived column '{target_column}' based on selected columns.\")\n",
    "\n",
    "# Drop rows with missing target values\n",
    "data = data.dropna(subset=[target_column])\n",
    "print(f\"Dataset shape after dropping rows with missing target: {data.shape}\")\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_columns = X.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "categorical_columns = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "print(\"\\nNumeric columns:\", numeric_columns)\n",
    "print(\"Categorical columns:\", categorical_columns)\n",
    "\n",
    "# Handle preprocessing with ColumnTransformer\n",
    "# - Impute missing values for numeric columns with the mean\n",
    "# - Encode categorical columns with OneHotEncoder\n",
    "# - Retain column names after transformation\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"mean\"), numeric_columns),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_columns),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# Apply the transformations\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "feature_names = (\n",
    "    numeric_columns.tolist()\n",
    "    + preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_columns).tolist()\n",
    ")\n",
    "\n",
    "X_preprocessed = pd.DataFrame(X_preprocessed, columns=feature_names)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_preprocessed), columns=feature_names)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display final dataset information\n",
    "print(\"\\nFinal dataset after preprocessing:\")\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"y_test distribution:\\n{y_test.value_counts()}\")\n",
    "\n",
    "# Step 1: Verify target variability and check if classes are missing\n",
    "print(\"Unique values in the target variable:\", y_train.unique())\n",
    "print(\"Target variable distribution in training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Step 2: Check for target class imbalance\n",
    "if len(y_train.unique()) == 1:\n",
    "    print(\"The target variable has only one class. Investigate the dataset further.\")\n",
    "else:\n",
    "    print(\"The target variable has multiple classes. Proceeding with the next steps.\")\n",
    "\n",
    "# Step 3: Perform feature selection to reduce dimensionality\n",
    "# Remove low variance features\n",
    "print(\"Performing Variance Threshold to remove low variance features...\")\n",
    "vt = VarianceThreshold(threshold=0.01)  # Keep features with variance > 0.01\n",
    "X_train_reduced = vt.fit_transform(X_train)\n",
    "X_test_reduced = vt.transform(X_test)\n",
    "\n",
    "print(\"Shape after Variance Threshold:\")\n",
    "print(\"X_train:\", X_train_reduced.shape)\n",
    "print(\"X_test:\", X_test_reduced.shape)\n",
    "\n",
    "# Step 4: Apply PCA for further dimensionality reduction (optional)\n",
    "print(\"Applying PCA for dimensionality reduction...\")\n",
    "pca = PCA(n_components=100)  # Keep 100 principal components\n",
    "X_train_pca = pca.fit_transform(X_train_reduced)\n",
    "X_test_pca = pca.transform(X_test_reduced)\n",
    "\n",
    "print(\"Shape after PCA:\")\n",
    "print(\"X_train:\", X_train_pca.shape)\n",
    "print(\"X_test:\", X_test_pca.shape)\n",
    "\n",
    "# Step 5: Address class imbalance (if applicable)\n",
    "# Apply SMOTE only if there are multiple classes\n",
    "if len(y_train.unique()) > 1:\n",
    "    print(\"Applying SMOTE to address class imbalance...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_pca, y_train)\n",
    "    print(\"Shape after SMOTE:\")\n",
    "    print(\"X_train_balanced:\", X_train_balanced.shape)\n",
    "    print(\"y_train_balanced distribution:\")\n",
    "    print(y_train_balanced.value_counts())\n",
    "else:\n",
    "    print(\"SMOTE not applied as there is only one class in the target variable.\")\n",
    "\n",
    "# Save preprocessed data for model training\n",
    "np.save(\"X_train_preprocessed.npy\", X_train_pca)\n",
    "np.save(\"X_test_preprocessed.npy\", X_test_pca)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)\n",
    "\n",
    "print(\"Preprocessing steps completed. Data saved for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df2a946-c666-4ff5-938c-400ff6a7855c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['diagnostic_summary_changes_phase', 'diagnostic_summary_changes_ptid',\n",
      "       'rid', 'diagnostic_summary_changes_viscode',\n",
      "       'diagnostic_summary_changes_viscode2',\n",
      "       'diagnostic_summary_changes_examdate',\n",
      "       'diagnostic_summary_changes_bcadas',\n",
      "       'diagnostic_summary_changes_bcmmse',\n",
      "       'diagnostic_summary_changes_bcmmsrec',\n",
      "       'diagnostic_summary_changes_bcnmmms',\n",
      "       ...\n",
      "       'biomarker_samples_bicsftrns', 'biomarker_samples_bicsffroz',\n",
      "       'biomarker_samples_bilppadate', 'biomarker_samples_bilpfldate',\n",
      "       'biomarker_samples_bilpspdate', 'biomarker_samples_id',\n",
      "       'biomarker_samples_siteid', 'biomarker_samples_userdate',\n",
      "       'biomarker_samples_update_stamp', 'cognitive_status'],\n",
      "      dtype='object', length=195)\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23705167-adf3-454d-a7de-be679a562113",
   "metadata": {},
   "source": [
    " Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3ea745-23bc-402a-b8fa-97e021444c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9567\n",
      "Precision: 0.9660\n",
      "Recall: 0.9008\n",
      "F1-Score: 0.9322\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Load the preprocessed data (assuming you saved it as .npy and .csv files)\n",
    "X_train = np.load('X_train_preprocessed.npy')\n",
    "X_test = np.load('X_test_preprocessed.npy')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_test = pd.read_csv('y_test.csv')\n",
    "\n",
    "# Define and train the classifier (using RandomForest as an example)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(clf, 'cognitive_status_predictor_model.pkl')\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "183bacf2-84c5-4c58-b7cf-be5b9ac31c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RandomForest...\n",
      "Saved RandomForest model successfully.\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "Training GradientBoosting...\n",
      "\n",
      "Training XGBoost...\n",
      "\n",
      "Model comparison:\n",
      "                Model  Accuracy  Precision    Recall  F1-Score\n",
      "0        RandomForest  0.956736   0.965983  0.900777  0.932241\n",
      "1  LogisticRegression  0.897607   0.860562  0.823533  0.841640\n",
      "2    GradientBoosting  0.882087   0.863559  0.763802  0.810623\n",
      "3             XGBoost  0.953459   0.947340  0.909708  0.928142\n",
      "\n",
      "Best model based on F1-Score: RandomForest with F1-Score: 0.9322\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load the preprocessed data\n",
    "X_train = np.load('X_train_preprocessed.npy')\n",
    "X_test = np.load('X_test_preprocessed.npy')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()  # Reshaping y_train to 1D\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()  # Reshaping y_test to 1D\n",
    "\n",
    "# Define the models to try\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "    \n",
    "    # Optionally save the best model\n",
    "    if model_name == 'RandomForest':  # Save the random forest model as an example\n",
    "        joblib.dump(model, 'cognitive_status_predictor_random_forest.pkl')\n",
    "        print(f\"Saved {model_name} model successfully.\")\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Find the best model based on F1-Score\n",
    "best_model = results_df.loc[results_df['F1-Score'].idxmax()]\n",
    "print(f\"\\nBest model based on F1-Score: {best_model['Model']} with F1-Score: {best_model['F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08406c66-d2d6-4d84-b46b-3e9865990831",
   "metadata": {},
   "source": [
    "Fine-tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e437e1d-cce3-48af-98a6-d060440d8399",
   "metadata": {},
   "source": [
    "Recommendations Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb893850-8c57-41e0-b013-173372ba58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_risk(prediction_prob):\n",
    "    if prediction_prob < 0.3:\n",
    "        return \"Low Risk\"\n",
    "    elif 0.3 <= prediction_prob < 0.7:\n",
    "        return \"Moderate Risk\"\n",
    "    else:\n",
    "        return \"High Risk\"\n",
    "recommendations = {\n",
    "    \"Low Risk\": {\n",
    "        \"Treatment\": [\"Follow a Mediterranean diet\", \"Engage in social activities\"],\n",
    "        \"Lifestyle\": [\"Play memory-enhancing games\", \"Exercise 30 minutes daily\"]\n",
    "    },\n",
    "    \"Moderate Risk\": {\n",
    "        \"Treatment\": [\"Consult a primary care physician\", \"Start cognitive stimulation therapy\"],\n",
    "        \"Lifestyle\": [\"Engage in aerobic exercises\", \"Avoid high-sugar diets\"]\n",
    "    },\n",
    "    \"High Risk\": {\n",
    "        \"Treatment\": [\"Neurologist consultation\", \"Consider medication for symptom management\"],\n",
    "        \"Lifestyle\": [\"Work with a caregiver\", \"Follow a structured daily routine\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d419c1-2574-44ac-aae2-99678be965d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Category: Moderate Risk\n",
      "Treatment Recommendations: ['Consult a primary care physician', 'Start cognitive stimulation therapy']\n",
      "Lifestyle Recommendations: ['Engage in aerobic exercises', 'Avoid high-sugar diets']\n"
     ]
    }
   ],
   "source": [
    "def generate_recommendations(prediction_prob):\n",
    "    risk_category = categorize_risk(prediction_prob)\n",
    "    treatment_recs = recommendations[risk_category][\"Treatment\"]\n",
    "    lifestyle_recs = recommendations[risk_category][\"Lifestyle\"]\n",
    "    \n",
    "    return risk_category, treatment_recs, lifestyle_recs\n",
    "\n",
    "# Example usage\n",
    "example_prob = 0.65  # Prediction probability for a sample patient\n",
    "category, treatment, lifestyle = generate_recommendations(example_prob)\n",
    "print(f\"Risk Category: {category}\")\n",
    "print(f\"Treatment Recommendations: {treatment}\")\n",
    "print(f\"Lifestyle Recommendations: {lifestyle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2c421f-2858-4dfe-a592-ee9c9ab5666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "cognitive_status_predictor_random_forest = joblib.load('cognitive_status_predictor_random_forest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c5abca-7f10-4931-a4a8-7be7fef94f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Risk Probability Risk Category  \\\n",
      "0              0.96     High Risk   \n",
      "1              0.94     High Risk   \n",
      "2              0.10      Low Risk   \n",
      "3              0.79     High Risk   \n",
      "4              0.90     High Risk   \n",
      "\n",
      "                           Treatment Recommendations  \\\n",
      "0  [Neurologist consultation, Consider medication...   \n",
      "1  [Neurologist consultation, Consider medication...   \n",
      "2  [Follow a Mediterranean diet, Engage in social...   \n",
      "3  [Neurologist consultation, Consider medication...   \n",
      "4  [Neurologist consultation, Consider medication...   \n",
      "\n",
      "                           Lifestyle Recommendations  \n",
      "0  [Work with a caregiver, Follow a structured da...  \n",
      "1  [Work with a caregiver, Follow a structured da...  \n",
      "2  [Play memory-enhancing games, Exercise 30 minu...  \n",
      "3  [Work with a caregiver, Follow a structured da...  \n",
      "4  [Work with a caregiver, Follow a structured da...  \n"
     ]
    }
   ],
   "source": [
    "def predict_and_recommend(model, X_test):\n",
    "    predictions = model.predict_proba(X_test)[:, 1]  # Probability of 'High Risk'\n",
    "    results = []\n",
    "\n",
    "    for prob in predictions:\n",
    "        category, treatment, lifestyle = generate_recommendations(prob)\n",
    "        results.append({\n",
    "            \"Risk Probability\": prob,\n",
    "            \"Risk Category\": category,\n",
    "            \"Treatment Recommendations\": treatment,\n",
    "            \"Lifestyle Recommendations\": lifestyle\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example: Generate recommendations for test data\n",
    "recommendation_results = predict_and_recommend(cognitive_status_predictor_random_forest, X_test_pca)\n",
    "print(recommendation_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e923b34e-2637-4525-8501-eaa11a263e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_results.to_csv(\"recommendations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee3715-8c66-43d3-8491-b2a77801b2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
